# ANGOFA LEVERAGING OFA EMBEDDING FACTORIZATION FOR ANGOLAN LANGUAGES

This repository contains code and resources for the paper titled "ANGOFA: Leveraging OFA Embedding Factorization for Angolan Languages". In this paper, we introduce the first set of multilingual Pretrained Language Models (PLMs) tailored for five Angolan languages using the Multi-Architecture Fine-Tuning (MAFT) approach. We compare PLMs developed through MAFT with and without informed embedding initialization, denoted as ANGOFA and ANGXLM-R, respectively.

We provide code implementations for both ANGOFA and ANGXLM-R models, along with resources for embedding initialization and continued pretraining. Additionally, Hugging Face models for both ANGOFA and ANGXLM-R are available on the model hub for easy access and experimentation.

## Folder Structure
- **Embedding_Initialization**: Folder containing materials related to embedding initialization.
- **Continued_Pretraining**: Folder containing materials related to continued pretraining.
- **code**: Folder containing the code for both ANGOFA and ANGXLM-R models.
  - **AngOFA**: Folder containing code related to the ANGOFA model.
  - **ANGXLM-R**: Folder containing code related to the ANGXLM-R model.

## Hugging Face Models
Both ANGOFA and ANGXLM-R models are available on Hugging Face's model hub for easy access and experimentation.

- **[AngOFA Model](https://huggingface.co/username/AngOFA)**: Description of the AngOFA model and instructions on how to use it.
- **[ANGXLM-R Model](https://huggingface.co/username/ANGXLM-R)**: Description of the ANGXLM-R model and instructions on how to use it.

## Usage
- Clone this repository to your local machine.
- Navigate to the respective code folders for ANGOFA and ANGXLM-R to find instructions on how to use the models.
- Refer to the paper in the `paper` folder for detailed information on the models and experiments conducted.

## Citation
If you find this work helpful, please consider citing our paper:
